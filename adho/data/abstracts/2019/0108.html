<?xml version="1.0"?>
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
		<!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)-->
		<title>I-Media-Cities: A Digital Ecosystem Enriching A Searchable Treasure Trove Of Audio Visual
      Assets </title>
		<meta name="author" content="Gabriella Scipione , Antonella Guidazzoli , Silvano Imboden , Giuseppe Trotta , Margherita Montanari , Maria Chiara Liguori , and Simona Caraceni"/>
		<meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"/>
		<meta name="DC.Title" content="I-Media-Cities: A Digital Ecosystem Enriching A Searchable Treasure Trove Of Audio Visual Assets"/>
		<meta name="DC.Type" content="Text"/>
		<meta name="DC.Format" content="text/html"/>
		<link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"/>
		<link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"/>
		<style>
 div.dh2019 { max-width:1000px; margin: auto;}
 body {
       font-family: verdana;
       font-size: 11px;
}
 p {
    font-family: verdana;
    font-size: 11px;
}
</style>
	</head>
	<body class="simple" id="TOP">
		<div class="dh2019">
			<img src="https://dev.clariah.nl/files/dh2019/logoos/EP_banner_DH20194.png" alt="Banner DH2019 and UU" width="100%"/>
			<hr/>
			<br/>
			<div class="stdheader autogenerated">
				<h1 class="maintitle">
					<span class="titlem">I-Media-Cities: A Digital Ecosystem Enriching A Searchable Treasure
            Trove Of Audio Visual Assets</span>
					<span class="titlem"/>
				</h1>
			</div>
			<div class="stdfooter autogenerated">
				<address>Gabriella Scipione (g.scipione@cineca.it), Cineca, Italy and Antonella Guidazzoli
          (a.guidazzoli@cineca.it), Cineca, Italy and Silvano Imboden (s.imboden@cineca.it), Cineca,
          Italy and Giuseppe Trotta (g.trotta@cineca.it), Cineca, Italy and Margherita Montanari
          (m.montanari@cineca.it), Cineca, Italy and Maria Chiara Liguori (m.liguori@cineca.it),
          Cineca, Italy and Simona Caraceni (s.caraceni@cineca.it), Cineca, Italy</address>
			</div>
			<br/>
			<div class="dhconvalidator-xml-link"/>
			<!--TEI front-->
			<!--TEI body-->
			<div class="DH-Heading" id="index.xml-body.1_div.1">
				<h2 class="DH-Heading">
					<span class="headingNumber">1. </span>
					<span class="head">
						<a id="id_docs-internal-guid-656f9cbd-7fff-8933-ae49-aa3d9e8d946d">
							<!--anchor-->
						</a>
          
           Introduction: Digital Cultural Ecosystems </span>
				</h2>
				<p>The paradigm of ecosystems for digital cultural contents, the so called DCEs (Digital
          Cultural Ecosystems), can facilitate and foster the process of democratization of
          knowledge. This process started in the 1990’s with the first applications developed by
          means of ICT tools applied to Cultural Heritage (Felicori and Guidazzoli, 2003). Open
          digital frameworks allow the access and the enrichment of cultural digital resources
          enabling new researches and studies. Moreover, these systems can bring cultural content to
          different audiences in innovative ways and include citizens in the process of content
          enrichment (Apollonio et al., 2017) (Paneva-Marinova et al., 2017). With respect to
          digital Cultural Heritage, citizens should be more than consumers     <span id="ftn1_return">
						<a class="notelink" title="ViMM Manifesto full version, ViMM project, retrieved on 3 April 2019." href="#ftn1">
							<sup>1</sup>
						</a>
					</span>
					<span style="color:#000000">. The audience participation should be improved at an active
            level, implementing user-oriented perspectives. </span>
				</p>
				<p>
					<span style="color:#000000">The same principle was at the basis of the I-Media-Cities
            project, with its aim of involving both researchers and the general public, not only as
            users of the platform, as mere performers of queries, but, instead, as active
            contributors by tagging and further enriching the digital contents delivered by the
            platform. </span>
				</p>
			</div>
			<div class="DH-Heading" id="index.xml-body.1_div.2">
				<h2 class="DH-Heading">2.     <span class="head">
						<a id="id_docs-internal-guid-ea3dc732-7fff-2ad8-0b25-787828ad136d">
							<!--anchor-->
						</a>
          
          The I-Media-Cities project </span>
				</h2>
				<p>The     <a class="link_ref" href="http://imediacities.eu/">I-Media-Cities</a>
        
         Horizon 2020
          project is the initiative of 9 European Film Libraries, 5 research institutions, 2
          technological providers and a specialist of digital business models to share, access to
          and valorise audiovisual (A/V) content about history and identity of European cities. A
          huge quantity of fictional and non-fictional AV works (from the end of the 19    <sup>th</sup>
        
        
          century onwards) has been made available for research and creative purposes, describing
          cities in all aspects, including physical transformation and social dynamics     <a id="id_docs-internal-guid-b138c720-7fff-3a10-9246-709c8b6ed7fd">
						<!--anchor-->
					</a>
        
         (Table n. 1). </p>
				<p>The platform, with its dynamic processing pipeline and automatic video analysis tools,
          enables the enrichment of the moving images meta-data (Fig. 1). By allowing also manual
          and automatic annotations of the A/V content the platform creates a new digital ecosystem
          (Caraceni et al., 2017) .</p>
				<div class="figure">
					<img src="Pictures/ee0f4f5bf0954512eb662e8551cdc59c.jpg" alt="" class="graphic" width="100%" height="NaN"/>
				</div>
				<p>
					<a id="id_docs-internal-guid-586d515e-7fff-f3a9-10c6-e744b38f4d18">
						<!--anchor-->
					</a>
        
        Fig. 1: IMC Movie Processing Pipeline </p>
				<p>
					<a id="id_docs-internal-guid-22df397f-7fff-2654-6780-860b1bb6a72b">
						<!--anchor-->
					</a>
        
        The innovative elements of the I-Media-Cities (IMC) platform are not limited to
          conveying the audio/visual contents of nine archives to a single collector and access
          point. Once collected, the A/V material is elaborated with a series of algorithms that
          automatically enrich it with a good set of meta-data. Nowadays it is a common activity for
          people to contribute to the enrichment of web contents with various kinds of data, in
          particular through social media. However, this manual activity is an extremely laborious
          process; especially when it regards, as in the case of the IMC project, the annotation of
          videos at shot or single frame level. Automatic meta-data, on the other hand, while not
          yet achieving the accuracy of a manual enrichment, is able to add a great deal of
          information. To this end, IMC has integrated a set of tools provided by Fraunhofer for the
          automatic analysis of video and images. These algorithms provide information on the
          quality of video, identify camera movements, such as zoom or pan, segment videos in
          various scenes (shot), and identify and recognize objects and various elements (object
          detection) that appear in the videos of IMC platform. The object detection tool, thanks to
          deep learning techniques, is able to identify the presence, for example, of people or
          means of transport or architectural or urban elements, differentiating between man or
          woman, adult or child; points out trams or carriages or bicycles; a square or a fountain,
          and so on (Fig. 2). </p>
				<div class="figure">
					<img src="Pictures/899027cbe49aff630f6fcd19c18918f8.png" alt="" class="graphic" width="100%" height="NaN"/>
				</div>
				<p>Figure 2: Visualisation of the object detection activity. Green detects objects with a
          confidence higher than 80%, yellow between 50 and 80% and red lower than 50%.</p>
				<p>This is a particularly interesting and complex procedure, which has to manage the
          analysis of sometimes scarred films digitized several years ago and dating back up to the
          end of the 19    <sup>th</sup>
        
         century.</p>
				<p>
					<a id="id_docs-internal-guid-a8999f54-7fff-e80a-757c-357b78aeb297">
						<!--anchor-->
					</a>
				</p>
				<div class="figure">
					<img src="Pictures/4d6aec4be3783e19eeb9205468610ea6.png" alt="" class="graphic" width="100%"/>
				</div>
				<p>
					<a id="id_docs-internal-guid-f338a788-7fff-2b65-6e9b-b7ebc5c9aedf">
						<!--anchor-->
					</a>
        
        Figure 3: IMC platform: video content item page displaying manual and automatic
          meta-data. Tags are colour coded (automatic, manual from researchers, manual from general
          users). </p>
				<p>
					<a id="id_docs-internal-guid-d725107d-7fff-30d1-7287-58ff4511dc44">
						<!--anchor-->
					</a>
        
        Since the automatic tools still retain a certain margin of error, a sophisticated
          online “frame by frame” analysis tool has been set up for the manual correction of
          inaccuracies on the shot detection, particularly useful on older videos (Fig. 3). </p>
				<p>
					<a id="id_docs-internal-guid-f8568485-7fff-b62c-1235-574bb8059042">
						<!--anchor-->
					</a>
				</p>
				<div class="figure">
					<img src="Pictures/bc7a5b664a96ee4e5144642c7dcbd6b5.png" alt="" class="graphic" width="100%"/>
				</div>
				<p>
					<a id="id_docs-internal-guid-f187d0ac-7fff-139f-baf8-f49e226de040">
						<!--anchor-->
					</a>
        
        Figure 4: IMC platform: geo-localised search results </p>
				<p>
					<a id="id_docs-internal-guid-fe0f445b-7fff-8a16-5d01-71ab9374ddb1">
						<!--anchor-->
					</a>
        
        The result of this process is the creation of data that are transformed into semantic
          data, directly understandable by people as well as by computers. The IMC meta-data model
          summarizes the descriptive meta-data from the archives, and then uploaded directly with
          the audiovisual materials, based primarily on the     <a class="link_ref" href="http://filmstandards.org/fsc/index.php/Main_Page">CEN EN19507 standard</a>
        
        ; and
          the data generated once the material has been loaded onto the platform. As mentioned,
          these meta-data can be the result of an automatic or manual enrichment process (Baraldi et
          al., 2016), modeled using the W3C Web annotation Data Model (WADM). </p>
				<p>
					<a id="id_docs-internal-guid-680302ba-7fff-162b-8984-c020109cd875">
						<!--anchor-->
					</a>
        
        In I-Media-Cities a semantic search engine was developed for processing the requests
          coming from the user interface and, by analysing all the available meta-data, it provides
          researchers and citizens with the search results. </p>
				<p>The meta-data system includes different types of annotation associated with different
          details:</p>
				<ul>
					<li class="item">original meta-data of the video or image, providing information at the
            level of the whole content;</li>
					<li class="item">automatic annotations at the level of a single segment of the video
            (scene or shot);</li>
					<li class="item">manual annotations at the level of the single segment of the video (scene
            or shot), tags and geotag (geolocalised annotation) with which the content has been
            enriched.</li>
				</ul>
				<p>
					<a id="id_docs-internal-guid-58f1759b-7fff-c8ea-9dfd-f4dc9cffe29f">
						<!--anchor-->
					</a>
				</p>
				<div class="ficcccccjelbcbkikfjnlnifjgigiifherlfbbljvrlfdc&#xA;				  gure">
					<img src="Pictures/009f8b7c11639887f99cbf5cf5282ad6.png" alt="" class="graphic" width="100%"/>
				</div>
				<p>
					<a id="id_docs-internal-guid-b82c2f44-7fff-bf07-81d7-24ee8cde9411">
						<!--anchor-->
					</a>
        
        Table n.1 - Contents delivered to IMC platform up to February 2019 </p>
				<p>
					<a id="id_docs-internal-guid-0f157b70-7fff-9f45-76d1-be5d40b457bd">
						<!--anchor-->
					</a>
        
        In particular, the automatic and manual annotations, being linked to the single
          segment of the video (scene or shot), allow a collection of scenes belonging to different
          videos where the same element can be traced: for example the tramways and traffic of the
          early 1900s (Fig. 5). </p>
				<p>
					<a id="id_docs-internal-guid-ced559d1-7fff-37c5-6230-7c090b46cd7c1">
						<!--anchor-->
					</a>
				</p>
				<div class="figure">
					<img src="Pictures/ad24806e3113adf423b8ee0ad8770df0.png" alt="" class="graphic" width="100%"/>
				</div>
				<p>
					<a id="id_docs-internal-guid-c27ec6b3-7fff-7381-da7d-800e6dd63880">
						<!--anchor-->
					</a>
        
        Figure 5: IMC platform: search by term </p>
				<p>The automatic annotations guarantee that all the audiovisual material is analysed and
          annotated homogeneously, at least on a common set of aspects, while the manual annotations
          provide an enrichment of detailed information, also in the form of textual notes,
          bibliographic references, links to other similar material, both internal and external to
          I-Media-Cities. At the moment, there are 59,457 manual annotations (with 1,708 different
          terms); 422,123 automatic annotations (with 78 different terms); 6,411 geotags for 1,091
          different places.     <br/>
        
        The performance of the IMC platform rely upon the High Performance
          Computing resources of Cineca, which allow the use of the most suitable hardware
          architectures for the different analysis algorithms applied. </p>
				<p>In order to adequately present the contents of the research, particular attention is paid
          to the development of the visual interface, which also allows to perform searches starting
          directly from the map on which all the AV contents are geolocated, delimiting the
          selection through a time bar (Fig. 4). In relation to these aspects, a user experience
          evaluation process is applied, in accordance with the Agile methodology, which pervades
          the project (Cohen et al., 2004). The Agile methodology implies an iterative approach,
          with several phases of development: check, correction, check and development (Tab. 2).</p>
				<div class="figure">
					<img src="Pictures/dae5d71544d67d3b4aa4aa46a6ffe9bc.jpg" alt="" class="graphic" width="100%"/>
				</div>
				<p>
					<a id="id_docs-internal-guid-4e166459-7fff-b933-5b38-2984406bb6ed">
						<!--anchor-->
					</a>
				</p>
				<p>Table 2: Agile methodology flow chart</p>
				<p>
					<a id="id_docs-internal-guid-386adb0d-7fff-2d4c-cdba-17f6840ac693">
						<!--anchor-->
					</a>
        
        During the first phase     <span style="font-style:italic">User and System
            requirements</span>
        
         were gathered by breaking the project and its requirements down into
          little parts of user functionalities, called     <span style="font-style:italic">user
            stories</span>
        
         and     <span style="font-style:italic">use cases,</span>
        
         and prioritizing
          them. Each Film Heritage Institution (FHI) and research partner provided a list of vision
          items and user stories that were categorised and grouped in     <span style="font-style:italic">use cases</span>
        
         by the Coordinator of the project and Cineca
          staff. During this process, the technical partners detailed each     <span style="font-style:italic">use case</span>
        
         with more technical information, such as
          technological enablers. </p>
				<p>A series of training sessions and a set of “living document” were curated in order to
          enable this Agile process among researchers and users and make them aware of what was
          already available and feasible. “Living documents” are continuously updated, following
          both the development of the project and the state of the art of the different areas on
          which the project insists.</p>
				<p>The final phase of the project foresees the opening of the platform to the wider public,
          and not only to researchers. The enrichment of contents can therefore take advantage of
          crowd-sourcing, obviously subject to a check by researchers and archives on all the
          information added     <span id="ftn2_return">
						<a class="notelink" title="Among the metadata sent by the archives and associated to each content there is a “right status” value, selected from the following list: - In copyrig…" href="#ftn2">
							<sup>2</sup>
						</a>
					</span>
        
        . The results of the research can be presented by archives and researchers as
          virtual exhibitions, set up in a 3D Web environment within the platform itself. </p>
			</div>
			<div class="DH-Heading" id="index.xml-body.1_div.3">
				<h2 class="DH-Heading">
					<span class="headingNumber">3. </span>
					<span class="head">
						<a id="id_docs-internal-guid-0fa87ba2-7fff-86cd-cb03-8d936769444b">
							<!--anchor-->
						</a>
          
          Conclusions </span>
				</h2>
				<p>
					<span style="color:#000000">I-Media-Cities meets two main requirements of Cultural
            Heritage sector: gathering information of different types and from different sources and
            enriching the original information with annotations, automatic or manual ones. Each
            multimedia content, image or video, is inserted in a process in which the asset is
            associated with the original </span>
					<span style="color:#000000">meta-data</span>
					<span style="color:#000000">belonging to the archive and, then, it is enriched with
            automatic </span>
					<span style="color:#000000">meta-data</span>
					<span style="color:#000000">extracted by different algorithms and further enhanced by
            manual annotations. In the next phase of the project a particular attention will be
            devoted in improving the general public user experience (Krug, 2014). The citizens will
            be able to visualize the public domain content of the archives, search, browse, annotate
            A/V content and share their discoveries.</span>
				</p>
				<p>
					<a id="id_docs-internal-guid-71566381-7fff-467a-6c46-3a9a56ef9f28">
						<!--anchor-->
					</a>
				</p>
				<div class="figure">
					<img src="Pictures/d1281749836ed13d5418a9049e796f11.png" alt="" class="graphic" width="100%" height="NaN"/>
				</div>
			</div>
			<!--TEI back-->
			<div class="bibliogr" id="index.xml-back.1_div.1">
				<h2>
					<span class="headingNumber">Appendix A </span>
				</h2>
				<div class="listhead">Bibliography</div>
				<ol class="listBibl">
					<li id="index.xml-bibl-w1842375aab3b3b1b1b3">
						<div class="biblfree">
							<span style="font-weight:bold">Apollonio, F. I., Rizzo, F., Bertacchi, S., Dall’Osso,
                G., Corbelli, A. and Grana, C.</span>
            
             (2017). SACHER: Smart Architecture for
              Cultural Heritage in Emilia Romagna. In Grana, C. and Baraldi, L. (eds), Digital
              Libraries and Archives, vol. 733. Cham: Springer International Publishing, pp. 142–56
              doi:10.1007/978-3-319-68130-6_12.
              http://link.springer.com/10.1007/978-3-319-68130-6_12 (accessed 4 March 2019). </div>
					</li>
					<li id="index.xml-bibl-w1842375aab3b3b1b1b5">
						<div class="biblfree">
							<span style="font-weight:bold">Baraldi, L., Grana, C. and Cucchiara, R.</span>
            
             (2016).
              Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic Deep Features.
              Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval - ICMR
              ’16. New York, New York, USA: ACM Press, pp. 23–29 doi:10.1145/2911996.2912012.
              http://dl.acm.org/citation.cfm?doid=2911996.2912012 (accessed 4 March 2019). </div>
					</li>
					<li id="index.xml-bibl-w1842375aab3b3b1b1b7">
						<div class="biblfree">
							<span style="font-weight:bold">Caraceni, S., Carpene, M., D’Antonio, M., Fiameni, G.,
                Guidazzoli, A., Imboden, S., Liguori, M. C., et al.</span>
            
             (2017). I-media-cities, a
              searchable platform on moving images with automatic and manual annotations.     <span style="font-style:italic">2017 23rd International Conference on Virtual System &amp;
                Multimedia (VSMM)</span>
            
            . Dublin: IEEE, pp. 1–8 doi:10.1109/VSMM.2017.8346274.
              https://ieeexplore.ieee.org/document/8346274/ (accessed 4 March 2019). </div>
					</li>
					<li id="index.xml-bibl-w1842375aab3b3b1b1b9">
						<div class="biblfree">
							<span style="font-weight:bold">Cohen, D., Lindvall, M. and Costa, P.</span>
            
             (2004). An
              Introduction to Agile Methods.     <span style="font-style:italic">Advances in
                Computers</span>
            
            , vol. 62. Elsevier, pp. 1–66 doi:10.1016/S0065-2458(03)62001-2.
              https://linkinghub.elsevier.com/retrieve/pii/S0065245803620012 (accessed 4 March
              2019). </div>
					</li>
					<li id="index.xml-bibl-w1842375aab3b3b1b1c11">
						<div class="biblfree">
							<span style="font-weight:bold">Felicori, M. and Guidazzoli, A.</span>
            
             (2003).
              Experiences in immersive graphics and creation of multimedia database for promoting
              and enhancing cultural heritage. Roma: IRI Management CRIC-Centro Ricerche su
              Innovazione e Competitività Eventi Progetti Speciali. </div>
					</li>
					<li id="index.xml-bibl-w1842375aab3b3b1b1c13">
						<div class="biblfree">
							<span style="font-weight:bold">Krug, S.</span>
            
             (2014).     <span style="font-style:italic">Don’t Make Me Think, Revisited: A Common Sense Approach to Web Usability</span>
            
            .
              Third edition. Berkeley, Calif.: New Riders. </div>
					</li>
					<li id="index.xml-bibl-w1842375aab3b3b1b1c15">
						<div class="biblfree">
							<span style="font-weight:bold">Paneva-Marinova, D., Pavlov, R. and Kotuzov, N.</span>
            
            
              (2017). Approach for Analysis and Improved Usage of Digital Cultural Assets for
              Learning Purposes.     <span style="font-style:italic">Cybernetics and Information
                Technologies</span>
            
            ,     <span style="font-weight:bold">17</span>
            
            (3): 140–51
              doi:10.1515/cait-2017-0035. </div>
					</li>
				</ol>
			</div>
			<!--Notes in [TEI]-->
			<div class="notes">
				<div class="noteHeading">Notes</div>
				<div class="note" id="ftn1">
					<span class="noteLabel">1. </span>
					<div class="noteBody">
						<a class="link_ref" href="https://www.vi-mm.eu/wp-content/uploads/2016/12/ViMM-Manifesto-Revised-Final-Revised-19-November.pdf">ViMM Manifesto full version</a>
          
          , ViMM project, retrieved on 3 April 2019. </div>
				</div>
				<div class="note" id="ftn2">
					<span class="noteLabel">2. </span>
					<div class="noteBody">
						<p>Among the metadata sent by the archives and associated to each content there is a
              “right status” value, selected from the following list:</p>
						<p>- In copyright </p>
						<p>- EU Orphan Work </p>
						<p>- In copyright - Educational use permitted </p>
						<p>- In copyright - Non-commercial use permitted </p>
						<p>- Public Domain </p>
						<p>- No Copyright - Contractual Restrictions </p>
						<p>- No Copyright - Non-Commercial Use Only </p>
						<p>- No Copyright - Other Known Legal Restrictions </p>
						<p>- No Copyright - United States </p>
						<p>- Copyright Undetermined </p>
						<p>All contents, whatever “right status” they have, can be visualised by everybody, even
              by the general users group. The archives, depending on the “right status”, can upload
              low resolution versions with watermarking.</p>
					</div>
				</div>
			</div>
			<hr/>
		</div>
	</body>
</html>
