<?xml version="1.0"?>
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
		<!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)-->
		<title>Word Embeddings for Processing Historical Texts</title>
		<meta name="author" content="Rachele Sprugnoli and Giovanni Moretti"/>
		<meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"/>
		<meta name="DC.Title" content="Word Embeddings for Processing Historical Texts"/>
		<meta name="DC.Type" content="Text"/>
		<meta name="DC.Format" content="text/html"/>
		<link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"/>
		<link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"/>
		<style>
 div.dh2019 { max-width:1000px; margin: auto;}
 body {
       font-family: verdana;
       font-size: 11px;
}
 p {
    font-family: verdana;
    font-size: 11px;
}
</style>
	</head>
	<body>
		<div class="dh2019">
			<img src="https://dev.clariah.nl/files/dh2019/logoos/EP_banner_DH20194.png" alt="DH2019 Utrecht University, The Netherlands" width="100%"/>
			<hr/>
			<br/>
			<div class="stdheader autogenerated">
				<h1 class="maintitle">Word Embeddings for Processing Historical Texts</h1>
			</div>
			<div class="stdfooter autogenerated">
				<address>Rachele Sprugnoli (rachele.sprugnoli@unitn.it), Fondazione Bruno Kessler, Italy and Giovanni Moretti (moretti@fbk.eu), Fondazione Bruno Kessler, Italy</address>
			</div>
			<br/>
			<div class="dhconvalidator-xml-link"/>
			<!--TEI front-->
			<!--TEI body-->
			<p>
				<div style="float:right; ">
					<img src="../posters/0668.jpg" style="width:300px;float:right;margin-left: 5px"/>
				</div>
				<span>In the last years, word embeddings have become important resources to deal with many Natural Language Processing (NLP) tasks (Collobert et al., 2011; Maas et al., 2011; Lample et al., 2016). Several pre-trained word vectors have been released generated with different algorithms but all based on a huge amount of contemporary texts, mainly news and Wikipedia pages but also Twitter posts and crawled web pages. </span>
			</p>
			<p>
				<a id="docs-internal-guid-e05750f8-7fff-c2e6-0f">
					<!--anchor-->
				</a>
				<span>The interest towards this type of distributional approach has recently emerged also in the Digital Humanities community as proved by the organization of dedicated workshops (e.g., </span>
				<a class="link_ref" href="http://dariah-tda.github.io/meeting/activity/workshop/2017/12/20/CfP-Workshop-on-Embeddings.html">
					<span>http://dariah-tda.github.io/meeting/activity/workshop/2017/12/20/CfP-Workshop-on-Embeddings.html</span>
				</a>
				<span>) and the publication of scientific articles on vectors built from historical or literary texts for tracking semantic shifts (Hamilton et al., 2016; Wohlgenannt et al., 2018; Leavy et al., 2018).</span>
			</p>
			<p>
				<span>This submission aims at expanding current research on historical word embeddings by presenting a set of English vectors pre-trained on a sub-part of the Corpus of Historical American English (COHA) (Davies, 2012) with three different algorithms. The subset of COHA we have chosen includes 36,856 texts of all the four available genres (fiction, newspaper, magazine, non-fiction) published between 1860 and 1939 for a total of more than 198 million words. We chose this specific time frame because we have a collection of travel writings of the same period of publication on which we planned to perform several NLP tasks as the one presented in the “Application” section below. In particular, this collection (</span>
				<a class="link_ref" href="https://sites.google.com/view/travelwritingsonitaly">
					<span>https://sites.google.com/view/travelwritingsonitaly</span>
				</a>
				<span>) contains both travel reports and guides published in a period of radical transformation in travel habits thanks to several technological, economic and sociological factors that led to the decline of the Grand Tour and the emergence of leisure-oriented travels. As for the applied models, we used the GloVe, fastText and Levy &amp; Goldberg's approaches (Pennington et al., 2014; Levy &amp; Goldberg, 2014; Grave et al., 2018). By adopting these three models, we cover different types of word representation: GloVe is based on linear bag-of-words contexts, fastText on a bag of character n-grams, and Levy &amp; Goldberg’s model on dependency parse-trees.</span>
			</p>
			<p>
				<span>Before applying these models, we lower-cased all the texts; tokenisation and dependency parsing (required by the Levy &amp; Goldberg approach) were then performed with Stanford CoreNLP (Manning et al., 2014). The training was done by considering all words appearing at least 10 times in the COHA sub-corpus and a context window size of 10. In the first phase, words are mapped to their frequency count, then a context vocabulary is created taking into consideration the context window. Our pre-trained word embeddings (called </span>
				<span style="font-style:italic">HistoGlove</span>
				<span>, </span>
				<span style="font-style:italic">HistoFast</span>
				<span> and </span>
				<span style="font-style:italic">HistoLevy</span>
				<span>) have 300 dimensions and are publicly available online (</span>
				<a class="link_ref" href="http://dh.fbk.eu/technologies/histo">
					<span>http://dh.fbk.eu/technologies/histo</span>
				</a>
				<span>). </span>
			</p>
			<div class="table">
				<table class="rules" style="border-collapse:collapse;border-spacing:0;">
					<caption>Table 1. Examples of the most 7 similar words as induced by different embeddings</caption>
					<tr>
						<td class="Normal_(Web)"/>
						<td class="Normal_(Web)">HistoFast</td>
						<td class="Normal_(Web)">HistoGlove</td>
						<td class="Normal_(Web)">HistoLevy</td>
						<td class="Normal_(Web)">Word2Vec</td>
					</tr>
					<tr>
						<td class="Normal_(Web)">gay</td>
						<td class="Normal_(Web)">merry, gayest, joyous, gaiety, gayly, gayety, light-hearted</td>
						<td class="Normal_(Web)">
							<span>merry, bright, joyous, cheerful, brillant, happy, flowers</span>
						</td>
						<td class="Normal_(Web)">merry, gorgeous, joyous, rosy, lively, bright, cheerful</td>
						<td class="Normal_(Web)">
							<span>lesbian, bisexual, lgbt, lesbians, women, sexual, gays, homoxesual</span>
						</td>
					</tr>
					<tr>
						<td class="Normal_(Web)">dancing</td>
						<td class="Normal_(Web)">
							<span>dance, danced, dances, dancers, dancer, walzing, dancin</span>
						</td>
						<td class="Normal_(Web)">dance, playing, danced, singing, music, dances, dancers</td>
						<td class="Normal_(Web)">singing, bathing, skating, swimming, feasting, wrestling, chattering</td>
						<td class="Normal_(Web)">dance, singing, dances, songs, dancers, ballroom, featuring</td>
					</tr>
					<tr>
						<td class="Normal_(Web)">woman</td>
						<td class="Normal_(Web)">
							<span>girl, madwoman, lady, irishwoman, husband, maid, she</span>
						</td>
						<td class="Normal_(Web)">girl, man, women, wife, she, husband, mother</td>
						<td class="Normal_(Web)">
							<span>girl, man, damsel, gentlewoman, englishwoman, youngster, creature</span>
						</td>
						<td class="Normal_(Web)">man, person, girl, child, women, children, men</td>
					</tr>
				</table>
			</div>
			<p>
				<span>Table 1 shows the top 7 similar words, in terms of cosine similarity, of a given set of target words (i.e., “gay”, “dancing”, and “woman”) as found in our three historical word embeddings and in Word2Vec trained on contemporary data (Mikolov et al. 2013). Among the words reported in Table 1, the main meaning shift is observed for “gay”, for which the reference to homosexuality is not present in the historical vectors. As for “dancing”, it is worth noticing that historical vectors brings out typical terms of the considered period (</span>
				<span style="font-style:italic">walzing</span>
				<span>) and that the dependency-based approach induce similarities having the same syntactic role (that is, other gerunds, i.e. </span>
				<span style="font-style:italic">singing, bathing, skating, swimming, feasting, wrestling, chattering</span>
				<span>): instead of finding words having high domain similarity, Levy &amp; Goldberg model finds words with high functional similarity (Turney, 2012), thus words behaving like the target word. Terms rarely used in contemporary texts are detected for the target word “woman” as well (see the visualization of the corresponding embeddings in HistoGlove in Figure 1): e.g. </span>
				<span style="font-style:italic">madwoman, damsel, gentlewoman</span>
				<span>. Social roles such as </span>
				<span style="font-style:italic">maid</span>
				<span> and </span>
				<span style="font-style:italic">wife</span>
				<span> does not appear in the list of the most similar words in Word2Vec, replaced by the neutral term </span>
				<span style="font-style:italic">person</span>
				<span>.</span>
			</p>
			<div class="figure">
				<img src="Pictures/d53c8fda04d61f572ea4b624dce5fd38.png" alt=" Visualization of the embeddings of “woman”. Image created with the Embedding Projector   " class="inline" style=" width:15.742708333333333cm; height:11.649605555555556cm;"/>
				<div class="caption">Figure 1. Visualization of the embeddings of “woman”. Image created with the Embedding Projector 
            <a class="link_ref" href="https://projector.tensorflow.org/">https://projector.tensorflow.org/</a>
				</div>
			</div>
			<p>
				<a id="docs-internal-guid-6b9aa1d5-7fff-a189-a5">
					<!--anchor-->
				</a>
				<span>Application</span>
			</p>
			<div class="figure">
				<img src="Pictures/41557a4750d1704fd7a5703d41a03f48.png" alt=" Place names automatically detected and then visualized using Carto, https://carto.com/" class="inline" style=" width:14.869583333333333cm; height:11.486752777777777cm;"/>
				<div class="caption">Figure 2. Place names automatically detected and then visualized using Carto, https://carto.com/</div>
			</div>
			<p>
				<span>Our embeddings can be useful resources for the development of NLP tools aiming at processing historical texts with neural architectures (Sprugnoli and Tonelli, 2019). For example, we applied them to the recognition of place names of different types (e.g. “Vesuvius”, “Venice”, “Forum Romanum”) in English historical travel writings on Italy (Sprugnoli, 2018). The deep learning architecture we adopted (Reimers and Gurevych, 2017), using a small set of in-domain training data (100,000 tokens), the HistoGlove embeddings and no feature engineering, outperformed both the CoreNLP CRF (</span>
				<span style="font-style:italic">Conditional random fields)</span>
				<span> model retrained with the same dataset and the same neural architecture employing bigger vector spaces pre-trained on contemporary texts. Our best model achieves a precision of 86.4, a recall of 88.5 and an F-measure of 87.5. Figure 2 displays the place names, related to the center of Florence, automatically detected in the tenth chapter of “Florence and Northern Tuscany with Genoa” (Hutton, 1908).</span>
				<a id="docs-internal-guid-3c8ac080-7fff-f98f-de">
					<!--anchor-->
				</a>
			</p>
			<!--TEI back-->
			<div class="bibliogr" id="index.xml-back.1_div.1">
				<h2>
					<span class="headingNumber">Appendix A </span>
				</h2>
				<div class="listhead">Bibliography</div>
				<ol class="listBibl">
					<li id="index.xml-bibl-w8918323aab3b3b1b1b3">
						<div class="biblfree">Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K. and Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug), pp.2493-2537.</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1b5">
						<div class="biblfree">Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y. and Potts, C. (2011). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1 (pp. 142-150). Association for Computational Linguistics.</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1b7">
						<div class="biblfree">Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C. (2016). Neural Architectures for Named Entity Recognition. In Proceedings of NAACL-HLT (pp. 260-270).</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1b9">
						<div class="biblfree">Pennington, J., Socher, R. and Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c11">
						<div class="biblfree">Grave, E., Bojanowski, P., Gupta, P., Joulin, A. and Mikolov, T. (2018). Learning word vectors for 157 languages. arXiv preprint arXiv:1802.06893.</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c13">
						<div class="biblfree">Hamilton, W.L., Leskovec, J. and Jurafsky, D., (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Vol. 1, pp. 1489-1501).</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c15">
						<div class="biblfree">Wohlgenannt, G., Chernyak, E., Ilvovsky, D., Barinova, A. and Mouromtsev, D. (2018). Relation Extraction Datasets in the Digital Humanities Domain and their Evaluation with Word Embeddings. In Proceedings of CICLING 2018, Hanoi, Vietnam.</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c17">
						<div class="biblfree">Leavy, S., Wade, K., Meaney, G. and Greene, D. (2018). Navigating Literary Text with Word Embeddings and Semantic Lexicons. In Proceedings of the Workshop on Computational Methods in the Humanities (COMHUM 2018).</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c19">
						<div class="biblfree">Davies, M. (2012). Expanding horizons in historical linguistics with the 400-million word Corpus of Historical American English. Corpora, 7(2), pp.121-157.</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c21">
						<div class="biblfree">Levy, O. and Goldberg, Y. (2014). Dependency-based word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (Vol. 2, pp. 302-308).</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c23">
						<div class="biblfree">Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. (2014). The Stanford CoreNLP Natural Language Processing Toolkit In 
                <span style="font-style:italic">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</span>
            
            , pp. 55-60.
          </div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c25">
						<div class="biblfree">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In 
                <span style="font-style:italic">Advances in neural information processing systems</span>
            
             (pp. 3111-3119).
          </div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c27">
						<div class="biblfree">Peter D. Turney. (2012). Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533–585.</div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c29">
						<div class="biblfree">Reimers, N., &amp; Gurevych, I. (2017). Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging. In 
                <span style="font-style:italic">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</span>
            
             (pp. 338-348).
          </div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c31">
						<div class="biblfree">Hutton, E. (1908). 
                <span style="font-style:italic">Florence and Northern Tuscany with Genoa</span>
            
            . Methuen.
          </div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c33">
						<div class="biblfree">Sprugnoli, R. and Tonelli, S. (2019). Novel Event Detection and Classification for Historical Texts. Computational Linguistics, Vol. 45, No. 2, June 2019, pp.1-38. </div>
					</li>
					<li id="index.xml-bibl-w8918323aab3b3b1b1c35">
						<div class="biblfree">Sprugnoli, Rachele. (2018). Arretium or Arezzo? A Neural Approach to the Identification of Place Names in Historical Texts. In 
                <span style="font-style:italic">Proceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)</span>
            
            , Torino, Italy, December 10-11, 2018.
          </div>
					</li>
				</ol>
			</div>
			<hr/>
		</div>
	</body>
</html>
