<?xml version="1.0"?>
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
		<!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)-->
		<title>A Transcription Portal for Oral History Research and Beyond</title>
		<meta name="author" content="Henk van den Heuvel , Christoph Draxler , Arjan van Hessen , Louise Corti , Stefania Scagliola , Silvia Calamai , and Norah Karouche"/>
		<meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"/>
		<meta name="DC.Title" content="A Transcription Portal for Oral History Research and Beyond"/>
		<meta name="DC.Type" content="Text"/>
		<meta name="DC.Format" content="text/html"/>
		<link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"/>
		<link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"/>
		<style>
 div.dh2019 { max-width:1000px; margin: auto;}
 body {
       font-family: verdana;
       font-size: 11px;
}
 p {
    font-family: verdana;
    font-size: 11px;
}
</style>
	</head>
	<body>
		<div class="dh2019">
			<img src="https://dev.clariah.nl/files/dh2019/logoos/EP_banner_DH20194.png" alt="DH2019 Utrecht University, The Netherlands" width="100%"/>
			<hr/>
			<br/>
			<div class="stdheader autogenerated">
				<h1 class="maintitle">A Transcription Portal for Oral History Research and Beyond</h1>
			</div>
			<div class="stdfooter autogenerated">
				<address>Henk van den Heuvel (h.vandenheuvel@let.ru.nl), Radboud University, the Netherlands and Christoph Draxler (draxler@phonetik.uni-muenchen.de), LMU, Munich, Germany and Arjan van Hessen (draxler@phonetik.uni-muenchen.de), University Twente, Enschede, the Netherlands and Louise Corti (corti@essex.ac.uk), University of Essex, UK and Stefania Scagliola (scagliolas@gmail.com), University of Luxembourg and Silvia Calamai (silvia.calamai@unisi.it), DSFUCI, University of Siena, Italy and Norah Karouche (karrouche@eshcc.eur.nl), Erasmus Studio, University of Rotterdam, the Netherlands</address>
			</div>
			<br/>
			<div class="dhconvalidator-xml-link"/>
			<!--TEI front-->
			<!--TEI body-->
			<div class="DH-Heading1" id="index.xml-body.1_div.1">
				<h2 class="DH-Heading1">
					<span class="headingNumber">1. </span>
					<span class="head">Background and Introduction</span>
				</h2>
				<p>Over the past 2 years a number of researchers from various backgrounds have been working on the exploitation of digital techniques and tools for working with oral history (OH) data. </p>
				<p>The result of a CLARIN workshop in Arezzo 
            <span id="ftn1_return">
						<a class="notelink" title="https://oralhistory.eu/workshops/arezzo" href="#ftn1">
							<sup>1</sup>
						</a>
					</span>
        
        , May 2017, was the idea of a so called Transcription Chain as a webportal where researcher could upload their audio files, have them transcribed by Automatic Speech Recognition (ASR) and could edit/correct the text results with a speech editor (Van den Heuvel et al., 2017).
      </p>
				<p>The Transcription Chain (TC) can be considered as a couple of concatenated different software tools that ingest Audio and or Video documents and output Time-stamped Transcriptions (TT) 
            <span id="ftn2_return">
						<a class="notelink" title="A Timed Transcription is a transcription of the spoken content where each transcribed object (mostly words, but, when possible, laughing, crying, and…" href="#ftn2">
							<sup>2</sup>
						</a>
					</span>
        
        . A TC can be a set of software packages stored and run on a personal computer, but in this proposal, we see a TC as a set of web based tools, running on one or more computer servers “in the internet”. A TC typically uses different tools that run on different servers in different countries.
      </p>
				<p>The TC as defined in the Arezzo-workshop contains two basic elements: Transcription and Alignment.</p>
				<p>Transcription of the (spoken) content of an AV-document can be done in two ways: </p>
				<ol>
					<li class="item">Automatically by an ASR-engine eventually followed by manual checking and correcting the recognition results</li>
					<li class="item">Manually, eventually followed by a forced alignment to receive a TT with the start- and end-times of all spoken words.</li>
				</ol>
				<p>After (post)editing of a transcription it needs to be (re)aligned with the speech signal. Several alignment tools can be used for this operation.</p>
				<p>The TC was implemented as a OH Transcription portal by developers of the Bavarian Archive for Speech Signals (BAS) in Munich. In this contribution we address the implementation of the portal (and its URL), the first experiences as reported in a follow-up CLARIN workshop in Munich (see also Van Hessen et al, 2019), and our future plans with the portal.</p>
			</div>
			<div class="DH-Heading1" id="index.xml-body.1_div.2">
				<h2 class="DH-Heading1">
					<span class="headingNumber">2. </span>
					<span class="head">Implementation of the Portal</span>
				</h2>
				<p>A prototype implementation of the OH portal has been set up at the Institute of Phonetics and Speech Processing. It can be accessed via the following URL:</p>
				<p>
					<a class="link_ref" href="https://www.phonetik.uni-muenchen.de/apps/oh-portal/">
						<span style="text-decoration:underline;color:1155CC">https://www.phonetik.uni-muenchen.de/apps/oh-portal/</span>
					</a>
				</p>
				<p>The portal works like a dynamic spreadsheet: the columns represent files and processes, the rows are individual files. Files enter the leftmost column, and then proceed from left to right through the spreadsheet. This way, one can monitor the progress of one’s data through the transcription chain. At every step in the processing can intermediate results be downloaded to the local machine.</p>
				<p>This transcription chain currently consists of four steps (see also Fig.1):</p>
				<ol>
					<li class="item">Data upload and verification</li>
					<li class="item">Automatic speech recognition (ASR)</li>
					<li class="item">Manual verification and correction of the recognised text</li>
					<li class="item">Automatic word and phoneme alignment and segmentation</li>
				</ol>
				<p>The upload and verification step transfer the audio data to the server and check the file format, e.g. convert stereo files to two mono files. Then, the user is asked to select the ASR language. Currently, the portal supports English, Dutch, Italian and German. ASR is performed by academic partners such as the University of Twente, Radboud University Nijmegen, Sheffield University, or European Media Lab, or commercial service providers such as Google. Note that most ASR service providers store the audio data and to use them to improve their services – this is a severe problem for privacy reasons.</p>
				<p>Steps 2, 3 and 4 are mandatory: if the results of the ASR are known to be very reliable, then the manual verification can be omitted. On the other hand, if for some reason ASR does not work for the given files, one can skip ASR and proceed to the manual transcription of the file directly. In some cases, fine-grained word alignment is not needed, and hence it can be switched off.</p>
				<p style="text-align:left;">Figure 1: Main screen of the TC portal showing the four processing steps 
            <img class="inline" src="Pictures/cfbf4c600331c7613490c0f99032715f.png" alt="" style=" width:16.51cm; height:5.891388888888889cm;"/>
				</p>
				<p>This is indicated by the checkbox in the column head.</p>
				<p style="text-align:left;">Figure 2: Output formats of the web interface 
            <img class="inline" src="Pictures/a63d8b1a87a3cbb977c8c55a77440889.png" alt="" style=" width:16.51cm; height:6.949722222222222cm;"/>
				</p>
				<p>It is the nature of a portal that many sites can access the portal server simultaneously, and that the portal does not perform the services itself. Instead, it calls external service providers, e.g. ASR services, passes the data to these servers, and processes the results. In the current beta version of the portal, queuing of incoming requests is a bottleneck, because the portal has to wait until one job has been processed before starting the next. Hence, the portal cannot estimate how long processing will take, it cannot inform its users about estimated time of termination, and it cannot reorder the queue to optimise throughput.</p>
				<p>The verification and correction of the ASR outcome is done using the Octra editor (Pömp, 2017). Octra features three different graphical user interfaces for efficient transcription. The innovative 2D editor displays longer signal fragments on the screen with good time resolution. Human transcribers set boundaries in signal pauses and then transcribe the signal fragments between these pauses. </p>
				<p style="text-align:left;">
					<span class="pagebreak" id="index.xml-pb-w6383859aab3b1b3c27b1">[Page]</span>
				</p>
				<p>Figure 3: OCTRA interface for manual correction of the transcriptions</p>
				<div class="figure">
					<img src="Pictures/71a3f155975b70aa0e47661045793c12.png" alt="" class="inline" style=" width:16.51cm; height:12.347222222222221cm;"/>
				</div>
				<p>Octra is fully integrated into the portal, so that files opened with Octra will automatically be sent back to the portal for the subsequent processing steps.</p>
			</div>
			<div class="DH-Heading1" id="index.xml-body.1_div.3">
				<h2 class="DH-Heading1">
					<span class="headingNumber">3. </span>
					<span class="head">User Experiences </span>
				</h2>
				<p>Most OH researchers indicated that they prefer flawless transcriptions because they use the textual results for the final analyses. Only a few indicated that they used the transcriptions to quickly find the audio passage in question. </p>
				<p>Another issue is that most of the recorded speech is not grammatically correct. However, solving this problem (the ungrammatical speech) is impossible because it would be tantamount to interpreting the text. Nonetheless, to increase the readability of the text, we tried to make “sentences” by adding a full stop after a pause of 400 msec or more. This isn’t a perfect solution but it made the text more “readable” according to the scholars in the Munich-workshop. The disadvantage is that you get a lot of short sentences when people speak hesitantly.</p>
				<p>Finally, we most recordings (interviews) contain multiple speakers. This is solved by speaker clustering: indicating when someone else was speaking. As a result, we get a transcript where a new paragraph is started each time the speaker changes. Diarization is not the same as speaker recognition, so we do not get a name or ID of the speaker but only an indication of the speaker (M1 is the first speaker, probably male). In general, we get more speakers than there are in reality.</p>
			</div>
			<div class="DH-Heading1" id="index.xml-body.1_div.4">
				<h2 class="DH-Heading1">
					<span class="headingNumber">4. </span>
					<span class="head">Future work</span>
				</h2>
				<p>The OH portal is currently being updated to improve throughput and to adapt to changes in ASR services. We plan to implement a traffic light system to distinguish ASR services by their privacy policy, file quotas and language support. Furthermore, an authentication mechanism will be installed.</p>
				<p>In the first official release (version 1.0) the commercial engines were removed but the participating scholars were informed about the inclusion of commercial engines in the previous releases. To our surprise, some scholars argued that they had no objection at all to the use of commercial software because they had already posted their OH-interviews on YouTube. So, in the next release we will include them again and offer the scholars the option to use them or not.</p>
				<p>Furthermore, we will extend the service to more languages. Contacts are established for Polish, Czech, Swedish and Finnish.</p>
				<p>Finally, scholars in Munich asked us if it will be possible in the near future to add their own vocabularies. At the moment it is not, but hopefully it will be possible in one of the next versions.</p>
			</div>
			<!--TEI back-->
			<div class="bibliogr" id="index.xml-back.1_div.1">
				<h2>
					<span class="headingNumber">Appendix A </span>
				</h2>
				<div class="listhead">Bibliography</div>
				<ol class="listBibl">
					<li id="index.xml-bibl-w6383859aab3b3b1b1b3">
						<div class="biblfree">
							<span style="font-weight:bold">Pömp, Draxler</span>
							<span> (2017) </span>
							<span style="font-style:italic">OCTRA – A configurable browser-based editor for orthographic transcription</span>
							<span>, Proceedings of Phonetik und Phonologie, pp. 145-148, Berlin, 2017</span>
						</div>
					</li>
					<li id="index.xml-bibl-w6383859aab3b3b1b1b5">
						<div class="biblfree">
							<span style="font-weight:bold">Van Hessen, Scagliola, Corti, Calamai, Karrouche, Draxler, Van den Heuvel, Beeken</span>
							<span> (2019) </span>
							<span style="font-style:italic">A Multidisciplinary Approach To The Use Of Technology In Research: The Case Of Interview Data</span>
							<span>. Proceedings DH 2019, Utrecht, these proceedings.</span>
						</div>
					</li>
					<li id="index.xml-bibl-w6383859aab3b3b1b1b7">
						<div class="biblfree">
							<span style="font-weight:bold">Van den Heuvel, van Hessen, Scagliola, Draxler</span>
							<span> (2017) </span>
							<span style="font-style:italic">Transcribing Oral History Audio Recordings – the Transcription Chain Workflow</span>
							<span>. Poster at EU. Clarin Conference, Budapest, September 18/19- 2017.</span>
						</div>
					</li>
				</ol>
			</div>
			<!--Notes in [TEI]-->
			<div class="notes">
				<div class="noteHeading">Notes</div>
				<div class="note" id="ftn1">
					<span class="noteLabel">1. </span>
					<div class="noteBody">
						<p>
							<a class="link_ref" href="https://oralhistory.eu/workshops/arezzo">
								<span style="text-decoration:underline;color:1155CC">https://oralhistory.eu/workshops/arezzo</span>
							</a>
						</p>
					</div>
				</div>
				<div class="note" id="ftn2">
					<span class="noteLabel">2. </span>
					<div class="noteBody">
						<p>
							<span> A Timed Transcription is a transcription of the spoken content where each transcribed object (mostly words, but, when possible, laughing, crying, and other non-verbal utterances) has a start-time and a duration or end-time.</span>
						</p>
					</div>
				</div>
			</div>
			<hr/>
		</div>
	</body>
</html>
