<?xml version="1.0"?>
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
		<!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)-->
		<title>Overcoming the Challenges of Optical Music Recognition of Early Music with Machine Learning</title>
		<meta name="author" content="Gabriel Vigliensoni , Alex Daigle , Eric Liu , Jorge Calvo-Zaragoza , Juliette Regimbal , Minh Anh Nguyen , Noah Baxter , Zoe McLennan , and Ichiro Fujinaga"/>
		<meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"/>
		<meta name="DC.Title" content="Overcoming the Challenges of Optical Music Recognition of Early Music with Machine Learning"/>
		<meta name="DC.Type" content="Text"/>
		<meta name="DC.Format" content="text/html"/>
		<link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"/>
		<link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"/>
		<style>
 div.dh2019 { max-width:1000px; margin: auto;}
 body {
       font-family: verdana;
       font-size: 11px;
}
 p {
    font-family: verdana;
    font-size: 11px;
}
</style>
	</head>
	<body>
		<div class="dh2019">
			<img src="https://dev.clariah.nl/files/dh2019/logoos/EP_banner_DH20194.png" alt="DH2019 Utrecht University, The Netherlands" width="100%"/>
			<hr/>
			<br/>
			<div class="stdheader autogenerated">
				<h1 class="maintitle">Overcoming the Challenges of Optical Music Recognition of Early Music with Machine Learning</h1>
			</div>
			<div class="stdfooter autogenerated">
				<address>Gabriel Vigliensoni (), and Alex Daigle (), and Eric Liu (), and Jorge Calvo-Zaragoza (), and Juliette Regimbal (), and Minh Anh Nguyen (), and Noah Baxter (), and Zoe McLennan (), and Ichiro Fujinaga (), </address>
			</div>
			<br/>
			<div class="dhconvalidator-xml-link"/>
			<!--TEI front-->
			<!--TEI body-->
			<p>
				<span style="font-weight:bold">Abstract</span>
			</p>
			<p>Several centuries of manuscript music sit on the shelves of libraries, churches, and museums around the globe. On-line digitization programs are opening these collections to a global audience, but digital images are only the beginning of true accessibility since the musical content of these images cannot be searched by computers. In the SIMSSA (Single Interface for Music Score Searching and Analysis) project (Fujinaga, Hankinson, and Cumming, 2014) we aim at teaching computers to read music and assemble the data on a single website. However, the automatic retrieval and encoding of music from score images has many complexities. In this paper, we describe our current workflow to perform end-to-end optical music recognition (OMR) of early music sources. </p>
			<p>
				<span style="font-weight:bold">1 Introduction</span>
			</p>
			<p>The process of reading, extracting, and encoding the content from digitized images of music documents is called optical music recognition (OMR). Despite more than 50 years of research, OMR is still a complex problem. Some characteristics of music notation—such as the graphical properties of musical objects and the layout and superimposition of musical features—make OMR a remarkably difficult problem (Bainbridge and Bell, 2001). The slow development in OMR, particularly when dealing with early music, also lies in the variability of documents. Since most OMR systems have been developed using heuristic approaches, they usually do not generalize well to documents of a different type. </p>
			<p>
				<span style="font-weight:bold">2 End-to-end OMR workflow for medieval and renaissance music</span>
			</p>
			<p>For high scalability, we are taking a machine learning-based approach to OMR. The computer is trained with a large number of examples for each category of musical element to be classified and creates a model. Once a model is created, it is used to classify new examples that the computer has not yet seen. We have implemented this approach in a workflow that performs OMR in medieval and renaissance music scores images. The workflow is divided into four stages: document analysis, symbol classification, music reconstruction and encoding, and symbolic score generation and correction. The entire workflow is depicted in Figure 
          <span style="color:FF1923">1</span>
      
      .
    </p>
			<div class="figure">
				<img src="Pictures/7c41907040f8c26d0c64b2a6e138e779.png" alt="" class="inline" style=" width:16.002cm; height:3.8152916666666665cm;"/>
			</div>
			<p>
				<span>Figure 1. End-to-end optical music recognition workflow for early music. Boxes indicate the software applications on each step. Human symbols indicate interactive, adaptive stages. </span>
			</p>
			<p>
				<span style="font-weight:bold">2.1 Document analysis</span>
			</p>
			<p>Digitized music scores are the input to the system and document analysis is applied to segment the music document into layers. We developed Pixel.js (Saleh et al., 2017), an open source, web-based, pixel-level classification application to label pixels into their corresponding category or to correct the output of other image segmentation processes. We use this tool interactively with a convolutional neural network (Calvo-Zaragoza et al., 2018) to segment the document into a number of user-defined layers. After a few iterations of training and classification for optimizing the classifier, we obtain a number of image files corresponding to the segmented layers of the original score. For example, these layers may contain notes, staff lines, lyrics, annotations, or ornamental letters. The recognition of the music symbols and the analysis of their relationship is achieved once the symbols are isolated and classified in the found layers.</p>
			<p>
				<span style="font-weight:bold">2.2 Symbol classification</span>
			</p>
			<p>The application we developed for the symbol classification stage is called Interactive Classifier (IC). IC is a web-based version of the Gamera classifier (Droettboom, MacMillan, and Fujinaga, 2003). We use it to automatically group the connected components of a specific layer into glyphs. Then, we manually label a series of these musical glyphs into classes. For neume music notation we implement neume-based and neume component-based classification. In either case, IC will extract a set of features for describing each of the neume or neume component classes and will model a classifier. With this model, new glyphs will be classified based on k-nearest neighbours. Once the symbols of the score are classified, we proceed to add their musical context and encode them into a symbolic music format. </p>
			<p>
				<span style="font-weight:bold">2.3 Music reconstruction and encoding</span>
			</p>
			<p>We obtain the pitches of neumes or neume components by finding their absolute position in the corresponding staves and use the recognized clef of each system to assign a relative pitch (Vigliensoni et al., 2011). The output of IC conveys the position and size of each musical element in the original image, and so we add this information to the estimated pitch as well as the staff number to which each neume belongs. Finally, this musical information is encoded into the Music Encoding Initiative (MEI) machine-readable symbolic music format (Roland, 2002). </p>
			<p>
				<span style="font-weight:bold">2.4 Symbolic score generation and correction</span>
			</p>
			<p>The last two stages of our OMR workflow, music reconstruction and encoding and symbolic score generation have a common interactive breakpoint for visualizing and correcting the output of the automatized OMR process. This human-driven checkpoint is embedded as a web-based interface called Neon (Neume Editor Online) (Burlet et al., 2012). Neon overlays the original music score image and the rendered version of the output of the OMR process. By visual inspection, the user can assess the differences between the layers, and manually add, edit, or delete music symbols in the browser. So far, however, corrections entered by the user are not fed back into the learning system, but they change the encoded music file output. </p>
			<p>
				<span style="font-weight:bold">2.5 Workflow management system</span>
			</p>
			<p>All the constituent parts of our OMR workflow are handled by Rodan (Hankinson, 2015), a distributed, collaborative, and networked adaptive workflow management system that allows specifying interactive and non-interactive tasks. </p>
			<p>
				<span style="font-weight:bold">3 Future work </span>
			</p>
			<p>In future iterations of the project we will focus on: (i) implementing a non-heuristic, machine learning-based approach for pitch finding (similar to the approach proposed by Pacha and Calvo-Zaragoza (2018)); (ii) appending neumes to syllables (since most neume notation is used to set music to an existing text); and (iii) devising a way of feeding back into the workflow the corrected output in Neon. We hope that this infrastructure, in combination with the proper teaching strategies and tactics developed by human teachers in the interfaces for training OMR systems (Vigliensoni, Calvo-Zaragoza, and Fujinaga, 2018), will enable the end-to-end recognition and encoding of music from music score images.</p>
			<!--TEI back-->
			<div class="bibliogr" id="index.xml-back.1_div.1">
				<h2>
					<span class="headingNumber">Appendix A </span>
				</h2>
				<div class="listhead">Bibliography</div>
				<ol class="listBibl">
					<li id="index.xml-bibl-w2168953aab3b3b1b1b3">
						<div class="biblfree">
							<span style="font-weight:bold">Bainbridge, D. and Bell, T.</span>
            
             (2001). The challenge of optical music recognition. 
                <span style="font-style:italic">Computers and the Humanities</span>
            
            , 
                <span style="font-weight:bold">35</span>
            
            (2): 95–121.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1b5">
						<div class="biblfree">
							<span style="font-weight:bold">Burlet, G., Porter, A., Hankinson, A. and Fujinaga, I.</span>
            
             (2012). Neon.js: Neume editor online. 
                <span style="font-style:italic">Proceedings of the 13</span>
							<sup style="font-style:italic">th</sup>
							<span style="font-style:italic"> International Society for Music Information Retrieval Conference</span>
            
            , pp. 121–6.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1b7">
						<div class="biblfree">
							<span style="font-weight:bold">Calvo-Zaragoza, J., Castellanos, F. J., Vigliensoni, G. and Fujinaga, I.</span>
            
             (2018). Deep neural networks for document processing of music score images. 
                <span style="font-style:italic">Applied Sciences</span>
            
            , 
                <span style="font-weight:bold">8</span>
            
            (5): 654–74.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1b9">
						<div class="biblfree">
							<span style="font-weight:bold">Droettboom, M., Macmillan, K. and Fujinaga, I.</span>
            
             (2003). The Gamera framework for building custom recognition systems. 
                <span style="font-style:italic">Proceedings of the 2003 Symposium on Document Image Understanding Technologies</span>
            
            , pp. 275–86.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1c11">
						<div class="biblfree">
							<span style="font-weight:bold">Fujinaga, I., Hankinson, A. and Cumming, J. E.</span>
            
             (2014). Introduction to SIMSSA (Single Interface for Music Score Searching and Analysis). 
                <span style="font-style:italic">Proceedings of the 1st International Workshop on Digital Libraries for Musicology</span>
            
            , pp. 1–3.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1c13">
						<div class="biblfree">
							<span style="font-weight:bold">Hankinson, A.</span>
            
             (2015). Optical music recognition infrastructure for large-scale music document analysis. Ph.D. Dissertation. McGill University, Montréal, QC.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1c15">
						<div class="biblfree">
							<span style="font-weight:bold">Pacha, A. and Calvo-Zaragoza, J.</span>
            
             (2018). Optical Music Recognition in Mensural Notation with Region-Based Convolutional Neural Networks. 
                <span style="font-style:italic">Proceedings of the 19</span>
							<sup style="font-style:italic">th</sup>
							<span style="font-style:italic"> International Society for Music Information Retrieval Conference</span>
            
            , pp. 23–7.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1c17">
						<div class="biblfree">
							<span style="font-weight:bold">Roland, P.</span>
            
             (2002). The music encoding initiative (MEI). 
                <span style="font-style:italic">Proceedings of the First International Conference on Musical Applications Using XML</span>
            
            , pp. 55–9.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1c19">
						<div class="biblfree">
							<span style="font-weight:bold">Saleh, Z., Zhang, K., Calvo-Zaragoza, J., Vigliensoni, G. and Fujinaga, I.</span>
            
             (2017). Pixel.js: Web-based pixel classification correction platform for ground truth creation. 
                <span style="font-style:italic">Proceedings of the 14th IAPR International Conference on Document Analysis and Recognition</span>
            
            , pp. 39–40.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1c21">
						<div class="biblfree">
							<span style="font-weight:bold">Vigliensoni, G., Burgoyne, J. A., Hankinson, A. and Fujinaga, I.</span>
            
             (2011). Automatic pitch recognition in printed square-note notation. 
                <span style="font-style:italic">Proceedings of the 12</span>
							<sup style="font-style:italic">th</sup>
							<span style="font-style:italic"> International Society for Music Information Retrieval Conference</span>
            
            , pp. 423–8.
          </div>
					</li>
					<li id="index.xml-bibl-w2168953aab3b3b1b1c23">
						<div class="biblfree">
							<span style="font-weight:bold">Vigliensoni, G., Calvo-Zaragoza, J. and Fujinaga, I.</span>
            
             (2018). An environment for machine pedagogy: Learning how to teach computers to read music. 
                <span style="font-style:italic">Proceedings of the IUI Workshop on Music Interfaces for Listening and Creation</span>
            
            .
          </div>
					</li>
				</ol>
			</div>
			<hr/>
		</div>
	</body>
</html>
